{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "from ku import generators as gr\n",
    "from ku import generic as gen\n",
    "from ku import image_utils as iu\n",
    "from ku import model_helper as mh\n",
    "from ku import image_augmenter as aug\n",
    "\n",
    "from munch import Munch\n",
    "import pandas as pd, numpy as np\n",
    "import pytest, shutil, os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_params = Munch(batch_size    = 2,\n",
    "                   data_path     = 'images',\n",
    "                   input_shape   = (224,224,5),\n",
    "                   inputs        = ['filename'],\n",
    "                   outputs       = ['score'],\n",
    "                   shuffle       = False,\n",
    "                   fixed_batches = True)\n",
    "\n",
    "ids = pd.read_csv(u'ids.csv', encoding='latin-1')\n",
    "\n",
    "np.all(ids.columns == ['filename', 'score'])\n",
    "np.all(ids.score == range(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(im, arg1, arg2):\n",
    "    return np.zeros(1) + arg1 + arg2\n",
    "\n",
    "gen_params_local = gen_params.copy()\n",
    "gen_params_local.process_fn = preproc\n",
    "gen_params_local.process_args  = {'filename': ['filename_args','filename_args']}\n",
    "gen_params_local.batch_size = 4\n",
    "\n",
    "ids_local = ids.copy()\n",
    "ids_local['filename_args'] = range(len(ids_local))\n",
    "\n",
    "g = gr.DataGeneratorDisk(ids_local, **gen_params_local)\n",
    "x = g[0]\n",
    "# gen.pretty(g)\n",
    "assert np.array_equal(np.squeeze(x[0][0].T), np.arange(4)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gr.DataGeneratorDisk(ids, **gen_params)\n",
    "print(isinstance(g[0][1], list))\n",
    "print(np.all(g[0][1][0] == np.array([[1],[2]])))\n",
    "\n",
    "gen.get_sizes(g[0])=='([array<2,224,224,3>], [array<2,1>])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_fn = lambda p: iu.resize_image(iu.read_image(p), (100,100))\n",
    "# g = gr.DataGeneratorDisk(ids, read_fn=read_fn, **gen_params)\n",
    "# gen.get_sizes(g[0]) =='([array<2,100,100,3>], [array<2,1>])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload(gen)\n",
    "# x = np.array([[1,2,3]])\n",
    "# print(gen.get_sizes(([x.T],1,[4,5])))\n",
    "# y = np.array([[1,[1,2]]])\n",
    "# print(gen.get_sizes(y))\n",
    "# z = [g[0],([2],)]\n",
    "# print(gen.get_sizes(z[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_params.inputs = ['filename', 'filename']\n",
    "g = gr.DataGeneratorDisk(ids, **gen_params)\n",
    "assert gen.get_sizes(g[0]) == '([array<2,224,224,3>, array<2,224,224,3>], [array<2,1>])'\n",
    "\n",
    "g.inputs_df = ['score', 'score']\n",
    "g.inputs = []\n",
    "g.outputs = []\n",
    "gen.get_sizes(g[0])\n",
    "\n",
    "g.inputs_df = [['score'], ['score','score']]\n",
    "\n",
    "assert gen.get_sizes(g[0]) == '([array<2,1>, array<2,2>], [])'\n",
    "\n",
    "g.inputs_df = []\n",
    "g.outputs = ['score']\n",
    "assert gen.get_sizes(g[0]) == '([], [array<2,1>])'\n",
    "\n",
    "g.outputs = ['score',['score']]\n",
    "with pytest.raises(AssertionError): g[0]\n",
    "\n",
    "g.outputs = [['score'],['score']]\n",
    "assert gen.get_sizes(g[0]) == '([], [array<2,1>, array<2,1>])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gen.H5Helper('data.h5', overwrite=True) as h:\n",
    "    data = np.expand_dims(np.array(ids.score), 1)\n",
    "    h.write_data(data, list(ids.filename))\n",
    "\n",
    "with gen.H5Helper('data.h5', 'r') as h:\n",
    "    data = h.read_data(list(ids.filename))\n",
    "    assert all(data == np.array([[1],[2],[3],[4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_params.update(data_path='data.h5', \n",
    "                  inputs=['filename'],\n",
    "                  batch_size=2)\n",
    "gen.pretty(gen_params)\n",
    "g = gr.DataGeneratorHDF5(ids, **gen_params)\n",
    "assert gen.get_sizes(g[0]) == '([array<2,1>], [array<2,1>])'\n",
    "\n",
    "g.inputs_df = ['score', 'score']\n",
    "g.inputs = []\n",
    "g.outputs = []\n",
    "assert gen.get_sizes(g[0]) == '([array<2,2>], [])'\n",
    "\n",
    "g.inputs_df = [['score'], ['score','score']]\n",
    "assert gen.get_sizes(g[0]) == '([array<2,1>, array<2,2>], [])'\n",
    "\n",
    "g.inputs_df = []\n",
    "g.outputs = ['score']\n",
    "assert gen.get_sizes(g[0]) == '([], [array<2,1>])'\n",
    "\n",
    "g.outputs = ['score',['score']]\n",
    "with pytest.raises(AssertionError): g[0]\n",
    "\n",
    "g.outputs = [['score'],['score']]\n",
    "assert gen.get_sizes(g[0]) == '([], [array<2,1>, array<2,1>])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'features': [1, 2, 3, 4, 5], 'mask': [1, 0, 1, 1, 0]}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "def filter_features(df):\n",
    "    return np.array(df.loc[df['mask']==1,['features']])\n",
    "\n",
    "gen_params.update(data_path = None, \n",
    "                  outputs   = filter_features,\n",
    "                  inputs    = [],\n",
    "                  inputs_df = ['features'],\n",
    "                  shuffle   = False,\n",
    "                  batch_size= 5)\n",
    "# gen.pretty(gen_params)\n",
    "\n",
    "g = gr.DataGeneratorHDF5(df, **gen_params)\n",
    "assert gen.get_sizes(g[0]) == '([array<5,1>], array<3,1>)'\n",
    "assert all(np.squeeze(g[0][0]) == np.arange(1,6))\n",
    "assert all(np.squeeze(g[0][1]) == [1,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.zeros((5,5,3))\n",
    "c = np.zeros((5,5,3))\n",
    "c[1:4,1:4,:] = 1\n",
    "\n",
    "assert np.array_equal(aug.cropout_patch(m, patch_size=(3,3), patch_position=(0.5,0.5), fill_val=1), c)\n",
    "\n",
    "m = np.zeros((256,256,3))\n",
    "plt.imshow(aug.cropout_random_patch(m.copy(), patch_size=(128,128), fill_val=1))\n",
    "plt.show()\n",
    "plt.imshow(aug.cropout_random_patch(m.copy(), patch_size=(128,128), fill_val=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ku import image_utils as iu\n",
    "assert isinstance(iu.ImageAugmenter(np.ones(1)), aug.ImageAugmenter)\n",
    "\n",
    "m = np.zeros((5,5,3))\n",
    "c = np.zeros((5,5,3))\n",
    "c[1:4,1:4,:] = 1\n",
    "\n",
    "assert np.array_equal(aug.cropout_patch(m, patch_size=(3,3), patch_position=(0.5,0.5), fill_val=1), c)\n",
    "assert np.array_equal(aug.ImageAugmenter(c).cropout((3,3), crop_pos=(0.5,0.5), fill_val=1).result, c)\n",
    "assert np.array_equal(aug.ImageAugmenter(c).cropout((3,3), crop_pos=(0.5,0.5), fill_val=0).result, m)\n",
    "\n",
    "assert np.array_equal(aug.ImageAugmenter(c).crop((3,3), crop_pos=(0.5,0.5)).result, np.ones((3,3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.zeros((5,5,3))\n",
    "ml, mr = [m]*2\n",
    "ml[0:2,0:2,:] = 1\n",
    "mr[0:2,-2:,:] = 1\n",
    "\n",
    "assert np.array_equal(iu.ImageAugmenter(m).fliplr().result, m)\n",
    "assert np.array_equal(iu.ImageAugmenter(ml).fliplr().result, mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(gr)\n",
    "\n",
    "def preproc(im, *arg):\n",
    "    if arg:\n",
    "        return np.zeros(im.shape) + arg\n",
    "    else:\n",
    "        return im\n",
    "\n",
    "gen_params_local = gen_params.copy()\n",
    "gen_params_local.update(process_fn = preproc,\n",
    "                        data_path = 'data.h5', \n",
    "                        inputs    = ['filename', 'filename1'],\n",
    "                        process_args = {'filename' :'args'},\n",
    "                        batch_size = 4,\n",
    "                        shuffle    = False)\n",
    "\n",
    "ids_local = ids.copy()\n",
    "ids_local['filename1'] = ids_local['filename']\n",
    "ids_local['args'] = range(len(ids_local))\n",
    "ids_local['args1'] = range(len(ids_local),0,-1)\n",
    "\n",
    "g = gr.DataGeneratorHDF5(ids_local, **gen_params_local)\n",
    "\n",
    "assert np.array_equal(np.squeeze(g[0][0][0]), np.arange(4))\n",
    "assert np.array_equal(np.squeeze(g[0][0][1]), np.arange(1,5))\n",
    "assert np.array_equal(np.squeeze(g[0][1]), np.arange(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.stack is much faster on float32, and still faster for float16 data\n",
    "data_elem = np.arange(100000, dtype=np.float32)\n",
    "data = [data_elem.copy() for i in range(10000)]\n",
    "\n",
    "with gen.Timer('stack, convert float32'):\n",
    "    data_new_stack = np.float32(np.stack(data))\n",
    "\n",
    "with gen.Timer('iterate, init float32'):\n",
    "    data_new = None\n",
    "    for i, d in enumerate(data):\n",
    "        if data_new is None:\n",
    "            data_new = np.zeros((len(data),)+d.shape, dtype=np.float32)\n",
    "        data_new[i, ...] = d\n",
    "\n",
    "assert np.array_equal(data_new, data_new_stack)\n",
    "gen.print_sizes(data_new)\n",
    "gen.print_sizes(data_new_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(gr)\n",
    "\n",
    "gen_params_ = gen_params.copy()\n",
    "gen_params_.process_fn = lambda im: [im, im+1]\n",
    "\n",
    "g = gr.DataGeneratorDisk(ids, **gen_params_)\n",
    "gen.print_sizes(g[0])\n",
    "assert np.array_equal(g[0][0][0], g[0][0][1]-1)\n",
    "assert np.array_equal(g[0][1][0], np.array([[1],[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fn(*args):\n",
    "    g = args[1]\n",
    "    score = np.float32(g.ids[g.ids.filename==args[0]].score)\n",
    "    return np.ones((3,3)) * score\n",
    "\n",
    "gen_params_local = gen_params.copy()\n",
    "gen_params_local.batch_size = 3\n",
    "gen_params_local.read_fn = read_fn\n",
    "gen_params_local.process_fn = lambda im: [im, im+1]\n",
    "\n",
    "g = gr.DataGeneratorDisk(ids, **gen_params_local)\n",
    "gen.print_sizes(g[0])\n",
    "print(g[0][0][1])\n",
    "assert np.array_equal(g[0][0][0], g[0][0][1]-1)\n",
    "assert np.array_equal(g[0][0][1][0,...], np.ones((3,3))*2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.ones((4,4))\n",
    "assert np.array_equal(aug.imshuffle(m, [2,2]), np.ones((4,4)))\n",
    "\n",
    "m[:,0] = 0\n",
    "assert np.sum(aug.imshuffle(m, [4,4])==0)==4\n",
    "assert np.array_equal(aug.imshuffle(m, [1,1]), m)\n",
    "\n",
    "m = np.zeros((2,2))\n",
    "m[0,0] = 1\n",
    "for _ in range(1000):\n",
    "    assert np.sum(aug.imshuffle_pair(m, m, [2,2]))<=2\n",
    "    assert np.sum(aug.imshuffle_pair(m, 1-m, [2,2]))>=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.ones((4,4))\n",
    "m2 = np.zeros((4,4))\n",
    "\n",
    "for _ in range(1000):\n",
    "    for ratio in [0,0.25,0.5,0.75,1]:\n",
    "        assert np.sum(aug.imshuffle_pair(m1, m2, [4,4], ratio)) == ratio*16\n",
    "\n",
    "assert np.sum(aug.imshuffle_pair(m1, m2, [1,4], 0.6)) == 8\n",
    "assert np.sum(aug.imshuffle_pair(m1, m2, [1,4], 0.7)) == 12\n",
    "assert np.sum(aug.imshuffle_pair(m1, m2, [1,4], 0.75)) == 12\n",
    "assert np.sum(aug.imshuffle_pair(m1, m2, [1,4], 0.8)) == 12\n",
    "assert np.sum(aug.imshuffle_pair(m1, m2, [1,4], 0.9)) == 16\n",
    "assert np.sum(aug.imshuffle_pair(m1, m2, [1,4], 1)) == 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.ones((4,4))\n",
    "m2 = np.zeros((4,4))\n",
    "\n",
    "mix1 = aug.imshuffle_pair(m1, m2, [2,2], flip=True)\n",
    "mix2 = aug.imshuffle_pair(m1, m2, [2,2], flip=False)\n",
    "\n",
    "assert np.sum(mix1) == np.sum(mix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10\n",
    "ids_defa = pd.read_csv(u'ids.csv', encoding='latin-1')\n",
    "fnames = np.concatenate([ids_defa.filename.values]*3)[:size]\n",
    "ids  = pd.DataFrame(dict(cats  = ['cat{}'.format(i) for i in range(size)],\n",
    "                        dogs  = ['dog{}'.format(i) for i in range(size)],\n",
    "                        image_name = fnames,\n",
    "                        group = [i//4 for i in range(10)]))\n",
    "\n",
    "gen_params = Munch(batch_size    = 1,\n",
    "                   inputs        = ['image_name'],\n",
    "                   outputs       = ['dogs'],\n",
    "                   data_path     = 'images',\n",
    "                   group_by      = 'group',\n",
    "                   shuffle       = False,\n",
    "                   fixed_batches = True)\n",
    "\n",
    "\n",
    "for batch_size, len_g in zip(range(1, 5), [10, 5, 5, 3]):\n",
    "    gen_params.batch_size = batch_size\n",
    "    g = gr.DataGeneratorDisk(ids, **gen_params)\n",
    "    # gen.print_sizes(g[0])\n",
    "#     print('num batches:',len(g))\n",
    "    assert len(g)==len_g\n",
    "    a = g.ids_index.groupby('batch_index').group_by.mean().values\n",
    "    b = g.ids_index.groupby('batch_index').group_by.last().values\n",
    "    assert np.array_equal(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_params.group_by = None\n",
    "for batch_size, len_g in zip(range(1, 5), [10, 5, 3, 2]):\n",
    "    gen_params.batch_size = batch_size\n",
    "    g = gr.DataGeneratorDisk(ids, **gen_params)\n",
    "#     print('num batches:',len(g))\n",
    "    assert len(g)==len_g\n",
    "#     display(g.ids_index)\n",
    "\n",
    "gen_params.fixed_batches = False\n",
    "for batch_size, len_g in zip(range(1, 5), [10, 5, 4, 3]):\n",
    "    gen_params.batch_size = batch_size\n",
    "    g = gr.DataGeneratorDisk(ids, **gen_params)\n",
    "#     print('num batches:',len(g))\n",
    "    assert len(g)==len_g\n",
    "#     display(g.ids_index)\n",
    "\n",
    "# g.ids_index.sort_values('batch_index')\n",
    "# iu.view_stack(gen.mapmm(g[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iu.resize_folder('images/', 'images_temp/',\n",
    "                 image_size_dst=(50,50), over_write=True)\n",
    "image_list = iu.glob_images('images_temp', verbose=False)\n",
    "assert image_list\n",
    "ims = iu.read_image_batch(image_list)\n",
    "assert ims.shape == (4, 50, 50, 3)\n",
    "\n",
    "failed_images, all_images = iu.check_images('images_temp/')\n",
    "assert len(failed_images)==0\n",
    "assert len(all_images)==4\n",
    "\n",
    "iu.save_images_to_h5('images_temp', 'images.h5', \n",
    "                     overwrite=True)\n",
    "with gr.H5Helper('images.h5') as h:\n",
    "    assert list(h.hf.keys()) == sorted(all_images)\n",
    "shutil.rmtree('images_temp')\n",
    "os.unlink('images.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_src='images/'\n",
    "path_dst='images_aug/'\n",
    "\n",
    "def process_gen():\n",
    "    for num_patch in [(i,j) for i in [1,2,4,8] for j in [1,2,4,8]]:\n",
    "        fn = lambda im: aug.imshuffle(im, num_patch)\n",
    "        yield fn, dict(num_patch=num_patch)\n",
    "        \n",
    "ids_aug, errors = iu.augment_folder(path_src, path_dst, \n",
    "                                    process_gen, verbose=False)\n",
    "\n",
    "assert len(errors)==0\n",
    "assert len(ids_aug)==64\n",
    "\n",
    "(image_path, ext) = os.path.split(ids_aug.iloc[0,:].image_path)\n",
    "_, file_names = iu.glob_images('{}{}/'.format(path_dst,image_path), split=True)\n",
    "\n",
    "first_group_names = list(ids_aug.groupby('num_patch'))[0][1].image_name\n",
    "assert sorted(first_group_names) == sorted(file_names)\n",
    "\n",
    "shutil.rmtree(path_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iu.resize_folder('images/', 'images1/', image_size_dst=(100,100), overwrite=True)\n",
    "\n",
    "gp = gen_params.copy()\n",
    "gp.inputs = ['filename']\n",
    "gp.group_names = ['images/']\n",
    "gp.data_path   = ''\n",
    "g = gr.DataGeneratorDisk(ids, **gp)\n",
    "assert gen.get_sizes(g[0]) == '([array<2,224,224,3>], [array<2,1>])'\n",
    "\n",
    "gp.group_names = ['images/', 'images1/']\n",
    "g = gr.DataGeneratorDisk(ids, **gp)\n",
    "assert gen.get_sizes(g[0]) == '([array<2,224,224,3>, array<2,100,100,3>], [array<2,1>])'\n",
    "\n",
    "gp.group_names = [['images/'], ['images1/']]\n",
    "sizes = []\n",
    "for i in range(100):\n",
    "    g = gr.DataGeneratorDisk(ids, **gp)\n",
    "    sizes.append(g[0][0][0].shape[1])\n",
    "\n",
    "assert np.unique(sizes).shape[0]>1\n",
    "\n",
    "!rm -R images1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iu.resize_folder('images/', 'base/images100/', image_size_dst=(100,100), overwrite=True)\n",
    "iu.resize_folder('images/', 'base/images50/', image_size_dst=(50,50), overwrite=True)\n",
    "\n",
    "gp = gen_params.copy()\n",
    "gp.inputs       = ['filename']\n",
    "gp.data_path    = ''\n",
    "gp.group_names  = ['base']\n",
    "gp.random_group = True\n",
    "g = gr.DataGeneratorDisk(ids, **gp)\n",
    "assert np.array_equal(np.unique([x[0][0].shape[1] \n",
    "                          for i in range(100) for x in g]), [50,100])\n",
    "    \n",
    "!rm -R base/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.DataFrame(dict(a = range(10), \n",
    "                        b = list(range(9,-1,-1))))\n",
    "gen_params = Munch(batch_size    = 4,\n",
    "                   data_path     = None,\n",
    "                   input_shape   = None,\n",
    "                   inputs_df     = ['a'],\n",
    "                   outputs       = ['b'],\n",
    "                   shuffle       = False,\n",
    "                   fixed_batches = True)\n",
    "\n",
    "# check fixed batches switch\n",
    "g = gr.DataGeneratorDisk(ids, **gen_params)\n",
    "assert np.array_equal([gen.get_sizes(x) for x in g], \n",
    "                      ['([array<4,1>], [array<4,1>])', \n",
    "                       '([array<4,1>], [array<4,1>])'])\n",
    "assert np.array_equal(g[0][0][0].squeeze(), range(4))\n",
    "\n",
    "gen_params.fixed_batches = False\n",
    "g = gr.DataGeneratorDisk(ids, **gen_params)\n",
    "assert np.array_equal([gen.get_sizes(x) for x in g], \n",
    "                      ['([array<4,1>], [array<4,1>])',\n",
    "                       '([array<4,1>], [array<4,1>])',\n",
    "                       '([array<2,1>], [array<2,1>])'])\n",
    "assert np.array_equal(g[2][0][0].squeeze(), [8, 9])\n",
    "\n",
    "# check randomized\n",
    "gen_params.shuffle = True\n",
    "gen_params.fixed_batches = False # maintain\n",
    "g = gr.DataGeneratorDisk(ids, **gen_params)\n",
    "\n",
    "# check if it returns all items\n",
    "data = list(zip(*list(g)))\n",
    "data0 = np.concatenate([l[0] for l in data[0]], axis=0).squeeze()\n",
    "data1 = np.concatenate([l[0] for l in data[1]], axis=0).squeeze()\n",
    "assert np.array_equal(np.sort(data0), np.arange(10))\n",
    "assert np.array_equal(np.sort(data1), np.arange(10))\n",
    "\n",
    "# check if randomization is applied, consistently\n",
    "num_randoms0 = 0\n",
    "num_randoms1 = 0\n",
    "for i in range(100):\n",
    "    g = gr.DataGeneratorDisk(ids, **gen_params)\n",
    "    data = list(zip(*list(g)))\n",
    "    data0 = np.concatenate([l[0] for l in data[0]], axis=0).squeeze()\n",
    "    data1 = np.concatenate([l[0] for l in data[1]], axis=0).squeeze()\n",
    "\n",
    "    # check consistency\n",
    "    ids_ = ids.copy()\n",
    "    ids_.index = ids_.a\n",
    "    np.array_equal(ids_.loc[data0].b, data1)\n",
    "\n",
    "    num_randoms0 += not np.array_equal(data0, np.arange(10))\n",
    "    num_randoms1 += not np.array_equal(data1, np.arange(10))\n",
    "\n",
    "# check randomizatino, at least once\n",
    "assert num_randoms0\n",
    "assert num_randoms0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from ku import applications as apps\n",
    "\n",
    "ids = pd.DataFrame(dict(a = np.arange(100), \n",
    "                        b = np.flip(np.arange(100))))\n",
    "ids = apps.get_train_test_sets(ids)\n",
    "# display(ids)\n",
    "\n",
    "X = Input(shape=(1,), dtype='float32')\n",
    "y = apps.fc_layers(X, name = 'head',\n",
    "                   fc_sizes      = [5, 1],\n",
    "                   dropout_rates = [0, 0],\n",
    "                   batch_norm    = 0)\n",
    "model = Model(inputs=X, outputs=y)\n",
    "\n",
    "gen_params = Munch(batch_size   = 4,\n",
    "                  data_path     = '',\n",
    "                  input_shape   = (1,),\n",
    "                  inputs_df     = ['a'],\n",
    "                  outputs       = ['b'])\n",
    "\n",
    "helper = mh.ModelHelper(model, 'test_model', ids, \n",
    "                        loss       = 'MSE',\n",
    "                        metrics    = ['mean_absolute_error'],\n",
    "                        monitor_metric = 'val_mean_absolute_error',\n",
    "                        multiproc  = False, workers = 2,\n",
    "                        logs_root  = 'logs',\n",
    "                        models_root= 'models',\n",
    "                        gen_params = gen_params)\n",
    "\n",
    "print('Model name:', helper.model_name(test='on'))\n",
    "helper.update_name()\n",
    "\n",
    "valid_gen = helper.make_generator(ids[ids.set == 'validation'], \n",
    "                                  shuffle     =  False)\n",
    "valid_gen.batch_size = len(valid_gen.ids)\n",
    "valid_gen.on_epoch_end()\n",
    "assert valid_gen.ids_index.batch_index.unique().size == 1\n",
    "\n",
    "helper.train(lr=1e-1, epochs=50, verbose=False, valid_in_memory=True);\n",
    "\n",
    "assert path.exists(helper.params.logs_root + '/' + helper.model_name())\n",
    "\n",
    "helper.load_model(); # best\n",
    "valid_best1 = helper.validate(verbose=1)\n",
    "\n",
    "helper.train(lr=1, epochs=10, verbose=False, valid_in_memory=True);\n",
    "\n",
    "# validate final model\n",
    "valid_res_fin = helper.validate(verbose=1)\n",
    "\n",
    "helper.load_model(); # best\n",
    "valid_best2 = helper.validate(verbose=1)\n",
    "\n",
    "if valid_res_fin['loss'] > valid_best1['loss']:\n",
    "    assert valid_best1['loss'] == valid_best2['loss']\n",
    "\n",
    "y_pred = helper.predict(valid_gen)\n",
    "y_true = ids[ids.set=='validation'].b.values\n",
    "_, _, val_mae, _ = apps.rating_metrics(y_true, y_pred, show_plot=False);\n",
    "print(valid_best2)\n",
    "assert np.abs(val_mae - valid_best2['mean_absolute_error']) < 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "res = (256, 192)\n",
    "archive_url = \"http://datasets.vqa.mmsp-kn.de/archives/koniq10k_{}x{}.tar\".format(*res)\n",
    "print('download URL:', archive_url)\n",
    "gen.download_archive(archive_url,'./')\n",
    "assert os.path.exists('256x192')\n",
    "assert len(glob.glob('256x192/*')) == 10373\n",
    "shutil.rmtree('256x192')\n",
    "\n",
    "archive_url = \"http://datasets.vqa.mmsp-kn.de/archives/koniq10k_{}x{}_test.zip\".format(*res)\n",
    "print('download URL:', archive_url)\n",
    "gen.download_archive(archive_url,'./')\n",
    "assert os.path.exists('256x192')\n",
    "assert len(glob.glob('256x192/*')) == 2015\n",
    "shutil.rmtree('256x192')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_params_local = gen_params.copy()\n",
    "ids_local = ids.copy()\n",
    "def ids_fn():\n",
    "    ids_local.score = -ids_local.score\n",
    "    return ids_local\n",
    "\n",
    "gen_params_local.ids_fn = ids_fn\n",
    "gen_params_local.batch_size = 4\n",
    "g = gr.DataGeneratorDisk(ids, **gen_params_local)\n",
    "x = g[0][1][0]\n",
    "g.on_epoch_end()\n",
    "y = g[0][1][0]\n",
    "assert np.array_equal(-x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter_env] *",
   "language": "python",
   "name": "conda-env-jupyter_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
